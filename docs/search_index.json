[["index.html", "Black Ochre Data Labs: Data Standards Chapter 1 Introduction 1.1 Rationale 1.2 How to use and contribute to this book", " Black Ochre Data Labs: Data Standards Sam Buckberry, Jimmy Breen 2022-04-06 Chapter 1 Introduction Note: This is a living document and is under continuous development and review. 1.1 Rationale In this era of leveraging big data and genomics for precision medicine, small populations such as Indigenous Australians are at high risk for under-representation or complete lack of representation. Our goal is to collect, curate and assemble robust datasets that encompass critical aspects of Indigenous Australian health to ensure equitable outcomes in precision medicine are possible. This includes biological, cultural and socio-economic data to investigate health outcomes that will deliver meaningful benefit to Indigenous Australians. However, with big data also come significant responsibilities concerning data storage, sovereignty and accessibility. Large collections of data on Indigenous Australians require carefully considered data architecture that maintains sovereignty based on community-defined priorities, protocols, and agreements, with concomitant attendance to matters of accountability and building trust. Here we outline Black Ochre Data Labs standards for data storage, access and sharing. The goal of these data standards is to document the policies, rules, and standards governing how data are stored, secured, arranged, integrated, and used in analyses and reporting. These standards are being developed in consultation with Indigenous Communities. Where possible, these standards will also align with the The Global Alliance for Genomics and Health GA4GH standards. A well-considered approach to data architecture is critical given the national importance of Indigenous genomics data security. 1.2 How to use and contribute to this book This reference book is primarily for recording specific data standards and practices used in Black Ochre Data Labs. Before you begin to move, store, compress or analyse data: Review the standards on directory structure, file names and compression. Follow the standards regarding the types of data you will be working with. If the file types and workflows you are working with have not been documented, please add a record. To contribute to these standards, please submit a pull request through the GitHub repo. Documentation on how to submit pull requests can be found here. "],["file-type-descriptors-definitions.html", "Chapter 2 File type descriptors &amp; definitions 2.1 Describing primary data file types 2.2 File type descriptor example", " Chapter 2 File type descriptors &amp; definitions This page provides definitions and guidance on the information that should be recorded for primary data files. Data that is clearly, simply and consistently described (named and defined) is much easier for users to identify, understand, use and share the data. This also ensures consistency across projects, inter-operability and is complementary to reproducible research practices. If you will be working with a file type that is not described in these standards, please add a record. 2.1 Describing primary data file types Biological data continue to be produced from an extensive array of assays and technologies. These data are often represented in unique file types and processed by specific software and algorithms. Below are descriptors that we aim to use for each file type. File type: This is the file format to be typically denoted by the file extension. Description: A brief description of the file format. Compression: The algorithm and/or method used to compress the file. Permissions: These are the user permission profiles for the groups and users that have access to the data, and the level of access Category 1: (Highest): Lead PI (Alex Brown) and Chief Data Scientist (Jimmy Breen) Category 2: Lead PI, Chief Data Scientist and Data Science team Category 3: Lead PI, Chief Data Scientist, Data Science team and TKI IT Team Category 4: (Lowest): Public Access frequency: How often this data type may require access. These descriptions are based on a subset of the Google Cloud storage class descriptors. Standard: Standard Storage is for data that is frequently accessed and/or stored for only brief periods of time. Standard data files should be able to be regenerated from Nearline or Archive data files without too much time or effort. Nearline: Nearline Storage is ideal for data you plan to read or modify on average once per month or less. Nearline Storage is appropriate for data backup and short-term archiving. Archive: highly durable storage for data archiving, backup, and disaster recovery. Only used for infrequently accessed data. Community access: Are their community custodians for these data? If so, who are they? Identifiable: Could this data be used to identify an individual (YES/NO). Location: Location(s) where the data should be stored. Notes: Any additional information that would assist users of the data. Sample: A sample of the file data with descriptions of each field. Useful if this file type is not a widely used and documented standard. 2.2 File type descriptor example For example, the description of CGmap file format for DNA methylation data would look like this: File type: CGmap Description: TSV file of stranded pileup base calls for cytosine positions in the reference genome DNA methylation data. Compression: gzip Permissions: Category 2 Access frequency: Nearline Community access: TBD Identifiable: No Location: server_name::/location/of/data/ Notes: This file type is output from BSSeeker2 and used by CGmap tools Sample: chr1 G 3000851 CHH CC 0.1 1 10 chr1 C 3001624 CHG CA 0.0 0 9 chr1 C 3001631 CG CG 1.0 5 5 Format descriptions (columns): (1) chromosome (2) nucleotide on Watson (+) strand (3) position (4) context (CG/CHG/CHH) (5) dinucleotide-context (CA/CC/CG/CT) (6) methylation-level = #_of_C / (#_of_C + #_of_T) (7) #_of_C (methylated C, the count of reads showing C here) (8) = #_of_C + #_of_T (all Cytosines, the count of reads showing C or T here) "],["conventions.html", "Chapter 3 Conventions 3.1 Directory structure 3.2 File naming 3.3 Date formats 3.4 File compression 3.5 Participant README file", " Chapter 3 Conventions 3.1 Directory structure Rationale: Virtually all of our raw data is generated from samples gifted Aboriginal and Torres Strait Islander peoples, and they must retain access to and control over their samples and data. The directory structure for raw and processed data, and any identifiable data that relates to a single individual should be stored in a directory structure with the individuals unique identifier as the higher level directory. In the event of a person requesting their data to be deleted, this should be possible by deleting the top level folder associated to their participant id. Within the directory of an individual, the data should be organised by assay type (WGS, DNAme, RNA-seq), and then the data types for each assay. It is ok to create symbolic links to files and folders within an individuals directory using the ln -s command. Directory structure: ├── ID ├── readme.md ├── WGS ├── raw ├── aligned ├── DNAme ├── raw ├── aligned ├── RNA ├── raw ├── aligned For example, the directory structure for individual with the id BODLAXXRD might look like this. ├── data ├── BODLAXXRD ├── readme.md ├── WGS ├── raw ├── BODLAXXRD_wgs_unaligned_1.cram ├── aligned ├── BODLAXXRD_wgs_aligned.cram ├── processed ├── BODLAXXRD_wgs.vcf ├── DNAme ├── raw ├── BODLAXXRD_DNAme_unaligned.cram ├── aligned ├── BODLAXXRD_DNAme_aligned.cram 3.2 File naming Three principles for file names: Machine readable Human readable Works well with default ordering Machine readable - Regular expression and globbing friendly names - Avoid spaces, punctuation, accented characters, case sensitivity - Easy to compute on - Deliberate use of delimiters. Use the underscore _ or dashes - to separate elements in file names. - NEVER USE SPACES. Many computer systems cannot handle spaces in file names, so NEVER use spaces! Human readable - File name contains information on the file content. Basically, make it easy to figure out what the file is based on the name. Default ordering Use names that will order by default using commands like ls. For example, if you use the YYYMMDD date format, ls would chronologically order the files. 20211122_my_sequences_R1.fastq.gz 20211122_my_sequences_R2.fastq.gz 20220126_my_sequences_R1.fastq.gz 20220126_my_sequences_R1.fastq.gz Or for example, prefixing your scripts with a number would enable natural ordering: 01_run_fastqc.sh 02_map_wgbs.sh 03_postmap_wgbs.sh 04_plot_map_stats.R 3.3 Date formats Use the format YYYYMMDD. This enables easy date sorting of files. Use ISO 8601 formatted dates where date and time values are ordered from the largest to smallest unit of time: year, month (or week) and day, minute, second. These formats enable easy chronological sorting. For example, 5 Jan 2012 would be 20120105 3.4 File compression All data files should be compressed using the method described for the file type in the standards for that assay. 3.5 Participant README file Each folder created with a participant id should be initialised with a REAMDME.md file. This file should used to record any relevant specifics for this sample. This could include details such as anomalies observed in a FASTQC report, below expected alignment rates, outlier on clustering, etc. "],["whole-genome-sequencing.html", "Chapter 4 Whole genome sequencing 4.1 Raw data 4.2 Mapped data 4.3 Processed data 4.4 VCF", " Chapter 4 Whole genome sequencing 4.1 Raw data 4.1.1 CRAM File type: CRAM Description: CRAM supports a wide range of lossless and lossy sequence data preservation strategies enabling users to choose which data should be preserved. CRAM is the genomics compression standard for GA4GH Compression: Genozip Permissions: Category 1 Community access: TBD Identifiable: Yes Location: Notes: 4.2 Mapped data 4.2.1 CRAM File type: CRAM Description: CRAM supports a wide range of lossless and lossy sequence data preservation strategies enabling users to choose which data should be preserved. CRAM is the genomics compression standard for GA4GH Compression: Genozip Permissions: Category 1 Community access: TBD Identifiable: Yes Location: Notes: When used with a reference genome, this exact reference genome file should be recorded. 4.3 Processed data 4.4 VCF File type: VCF Description: Compression: Permissions: Access frequency: Community access: Identifiable: Location: Notes: "],["dna-methylation.html", "Chapter 5 DNA methylation 5.1 Raw data 5.2 Mapped data 5.3 Processed data", " Chapter 5 DNA methylation 5.1 Raw data 5.1.1 FASTQ File type: FASTQ Description: Compression: Permissions: Access frequency: Community access: Identifiable: Location: Notes: 5.2 Mapped data 5.2.1 CRAM File type: CRAM Description: Genomic alignments (typically). CRAM supports a wide range of lossless and lossy sequence data preservation strategies enabling users to choose which data should be preserved. CRAM is the genomics compression standard for GA4GH Compression: NA Permissions: Community access: Location: Notes: When used with a reference genome, this exact reference genome file should be recorded. CRAM specification 5.3 Processed data 5.3.1 CGmap File type: CGmap Description: TSV file of stranded pileup base calls for cytosine positions in the reference genome DNA methylation data. Compression: gzip Permissions: Access frequency: Community access: Identifiable: Location: Notes: This file type is output from BSSeeker2 and used by CGmap tools Sample: chr1 G 3000851 CHH CC 0.1 1 10 chr1 C 3001624 CHG CA 0.0 0 9 chr1 C 3001631 CG CG 1.0 5 5 Format descriptions (columns): (1) chromosome \\ (2) nucleotide on Watson (+) strand \\ (3) position \\ (4) context (CG/CHG/CHH) \\ (5) dinucleotide-context (CA/CC/CG/CT) \\ (6) methylation-level = #_of_C / (#_of_C + #_of_T) \\ (7) #_of_C (methylated C, the count of reads showing C here) \\ (8) = #_of_C + #_of_T (all Cytosines, the count of reads showing C or T here) 5.3.2 ATCGmap File type: ATCGmap Description: TSV file of stranded pileup base calls for DNA methylation data. Compression: gzip Permissions: Access frequency: Community access: Identifiable: YES Location: Notes: This file type is output from BSSeeker2 and used by CGmap tools Sample: chr1 T 3009410 -- -- 0 10 0 0 0 0 0 0 0 0 na chr1 C 3009411 CHH CC 0 10 0 0 0 0 0 0 0 0 0.0 chr1 C 3009412 CHG CC 0 10 0 0 0 0 0 0 0 0 0.0 chr1 C 3009413 CG CG 0 10 50 0 0 0 0 0 0 0 0.83 Format descriptions (columns): (1) chromosome \\ (2) nucleotide on Watson (+) strand \\ (3) position \\ (4) context (CG/CHG/CHH) \\ (5) dinucleotide-context (CA/CC/CG/CT) \\ (6) - (10) plus strand \\ (6) # of reads from Watson strand mapped here, support A on Watson strand \\ (7) # of reads from Watson strand mapped here, support T on Watson strand \\ (8) # of reads from Watson strand mapped here, support C on Watson strand \\ (9) # of reads from Watson strand mapped here, support G on Watson strand \\ (10) # of reads from Watson strand mapped here, support N \\ (11) - (15) minus strand \\ (11) # of reads from Crick strand mapped here, support A on Watson strand and T on Crick strand \\ (12) # of reads from Crick strand mapped here, support T on Watson strand and A on Crick strand \\ (13) # of reads from Crick strand mapped here, support C on Watson strand and G on Crick strand \\ (14) # of reads from Crick strand mapped here, support G on Watson strand and C on Crick strand \\ (15) # of reads from Crick strand mapped here, support N \\ (16) methylation_level = #C/(#C+#T) = C8/(C7+C8) for Watson strand, =C14/(C11+C14) for Crick strand \\ &quot;nan&quot; means none reads support C/T at this position. \\ 5.3.3 Bigwig File type: Bigwig Description: Compression: Permissions: Access frequency: Community access: Identifiable: Location: Notes: "],["reproducible-research-practice.html", "Chapter 6 Reproducible research practice 6.1 Standardised workflows 6.2 Reproducible data analyses", " Chapter 6 Reproducible research practice 6.1 Standardised workflows 6.2 Reproducible data analyses "],["how-to-contribute.html", "Chapter 7 How to contribute", " Chapter 7 How to contribute To contribute to these standards, please submit a pull request through the GitHub repo. Documentation on how to submit pull requests can be found here. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
